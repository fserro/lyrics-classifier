{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (Or: *What is unique about working with text*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How is text different from the types of data you've seen so far?\n",
    "* It's a string, so can't do mathematical operations on it\n",
    "* It can be ambigious\n",
    "* It can be in a wide range of formats / files\n",
    "* Has a wide granularity level (file, paragraph, sentence, word, character)\n",
    "* It is unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What does this mean for how you can work with text compared to types of data you've worked with so far?\n",
    "* Need to make it structured\n",
    "* Need to clean it well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main aspect of working with text that helps us feed it into computers / machine learning models:\n",
    "1. Data/text preprocessing\n",
    "2. Turning text into features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know that data cleaning is a large part of a data scientist's workflow. When working with unstructured data such as text, data cleaning plays an even bigger role.\n",
    "\n",
    "**Q**: Things we may want to do to clean our textual data:\n",
    "* Lemmatization\n",
    "* Stemming\n",
    "* Tokenization\n",
    "* Remove small words that don't contribute to the meaning\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the corpus into individual words / tokens\n",
    "* Remove punctuation\n",
    "* Deal with capitalization\n",
    "* Remove most common words:\n",
    "    * Use a list of words to remove (stop words)\n",
    "    * Remove the words that appear in more than X% of documents\n",
    "* Reduce words to their base parts:\n",
    "    * *Stemming*: process of removing and replacing suffixes to get to the root of the words, *stem*.\n",
    "        * based on heuristics (e.g. *-ational -> -ate*, *-tional -> -tion*)\n",
    "        * does not always produce a word\n",
    "        * feet->feet, wolves->wolv, cats->cat, talked->talk\n",
    "    * *Lemmatization*: uses vocabulary and morphological analysis to return the base or dictionary form of a word, *lemma*.\n",
    "        * does not always return a reduced form\n",
    "        * feet->foot, wolves->wolf, cats->cat, talked->talked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"we all love a yellow submarine\",             # Beatles\n",
    "          \"yesterday, my submarine was in love\",        # Beatles\n",
    "          \"we are love trouble with loyalty here\",      # Eminem\n",
    "          \"loyalty to us is worth more than love is\"]   # Eminem\n",
    "labels = ['Beatles'] * 2 + ['Eminem'] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Turning text into features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Once you have your data cleaned and preprocessed like this, what can you imagine using as your features?\n",
    "* Amount of certain words\n",
    "* Length of song / words in song\n",
    "* Categorize words, then use word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token/word will be a feature/column.\n",
    "* large sparse matrix\n",
    "* word order lost\n",
    "* counts not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: How can we remove the most common words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a list of stop words\n",
    "* Removing the words that appear in more than X% of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `CountVectorizer` documentation for how to do each of these: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove most common words using these two methods. Use `.vocabulary_` and `.stop_words_` attributes to see which words have remained and which are removed (latter only in the case of the second method). \n",
    "\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing the words that appear in more than X% of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.75)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of single tokens, we now also count token pairs (bigrams), triplets (trigrams), etc.\n",
    "* even larger sparser matrix\n",
    "* preserves local word order\n",
    "* counts not normalized\n",
    "* too many features:\n",
    "    * remove high-frequency n-grams: can include stop words; not very informative\n",
    "    * remove low-frequency n-grams: typos and rare n-grams; likely to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stands for `term frequency - inverse document frequency` and aims to address the popularity/frequency of words in a corpus(not just inside of a single document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF = term frequency \n",
    "\n",
    "TF(t, d) - frequency of term (n-gram) _t_ in document _d_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IDF(t) = inverse document frequency (of term _t_ in the whole corpus)\n",
    "\n",
    "$ IDF(t) = \\log \\frac{1+N}{1+N_t}+1 $\n",
    "\n",
    "\n",
    "If term _t_ doesn't appear in many documents: IDF is \"big\".\n",
    "\n",
    "If term _t_ appears in many documents: IDF is close to 1 (\"small\") -> common terms are penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ TFIDF(t, d) = TF(t,d)*IDF(t) $ \n",
    "\n",
    "**Q**: What kind of terms will have high TF-IDF?\n",
    "* Those that appear a lot in small number of documents/songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Implement TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up how to implement TF-IDF vectorizer in `scikit-learn`. How does your features dataframe differ from the `CountVectorizer` one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus question*: What can you say about the values in your new `X_df` (think about sums, normalizations, etc.)? Post your guesses in Slack! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: unlike `CountVectorizer`, using `TfidfVectorizer` results in normalized counts. What is normalized is the sum of squares, meaning that sum of squares in each row adds up to 1.\n",
    "\n",
    "You can check this yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(X_df).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your scraped lyrics and run them through a vectorizer of your choice. Then split the resulting feature vector `X` and your labels into training and test set and train a logistic regression model. Check how the choice of vectorizers and the parameters we mentioned affects the performance of your model.\n",
    "\n",
    "Once you have your final model, run the following lines of code to get the words that are strongest predictors for each of your bands and post them in Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import operator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model = LogisticRegression()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(operator.itemgetter(*np.argsort(model.coef_[0]))(vectorizer.get_feature_names())[-20:])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(operator.itemgetter(*np.argsort(model.coef_[0]))(vectorizer.get_feature_names())[:20])`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
